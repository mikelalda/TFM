{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4ba266d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1dcab2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(587, 96, 128, 3)\n",
      "(572, 96, 128, 3)\n",
      "(587, 96, 128, 3)\n",
      "(572, 96, 128, 3)\n",
      "(587, 96, 128, 38)\n",
      "(572, 96, 128, 38)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import transform\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "imsize = (128,96)\n",
    "\n",
    "x1_train = []\n",
    "for archivo in os.listdir('dataset_sunrgbd/train_rgb'):\n",
    "    img = Image.open(os.path.join('dataset_sunrgbd/train_rgb',archivo))\n",
    "    img = cv2.resize(img_to_array(img), dsize=imsize)\n",
    "    x1_train.append(np.asarray(img))\n",
    "x1_train = np.array(x1_train)\n",
    "x1_train = x1_train/255.0\n",
    "print(x1_train.shape)\n",
    "\n",
    "x1_test = []\n",
    "for archivo in os.listdir('dataset_sunrgbd/test_rgb'):\n",
    "    img = Image.open(os.path.join('dataset_sunrgbd/test_rgb',archivo))\n",
    "    img = cv2.resize(img_to_array(img), dsize=imsize)\n",
    "    x1_test.append(np.asarray(img))\n",
    "x1_test = np.array(x1_test)\n",
    "x1_test = x1_test/255.0\n",
    "print(x1_test.shape)\n",
    "\n",
    "x2_train = []\n",
    "for archivo in os.listdir('dataset_sunrgbd/train_depth'):\n",
    "    img = Image.open(os.path.join('dataset_sunrgbd/train_depth',archivo))\n",
    "    img = cv2.resize(cv2.cvtColor(img_to_array(img),cv2.COLOR_GRAY2RGB), dsize=imsize)\n",
    "#     img = cv2.cvtColor(img_to_array(img),cv2.COLOR_GRAY2RGB)\n",
    "    x2_train.append(np.asarray(img))\n",
    "x2_train = np.array(x2_train)\n",
    "x2_train = x2_train/26000\n",
    "print(x2_train.shape)\n",
    "\n",
    "x2_test = []\n",
    "for archivo in os.listdir('dataset_sunrgbd/test_depth'):\n",
    "    img = Image.open(os.path.join('dataset_sunrgbd/test_depth',archivo))\n",
    "    img = cv2.resize(cv2.cvtColor(img_to_array(img),cv2.COLOR_GRAY2RGB), dsize=imsize)\n",
    "#     img = cv2.cvtColor(img_to_array(img),cv2.COLOR_GRAY2RGB)\n",
    "    x2_test.append(np.asarray(img))\n",
    "x2_test = np.array(x2_test)\n",
    "x2_test = x2_test/26000.0\n",
    "print(x2_test.shape)\n",
    "\n",
    "y_train = []\n",
    "for archivo in os.listdir('dataset_sunrgbd/train_label'):\n",
    "    img = np.load(os.path.join('dataset_sunrgbd/train_label',archivo))\n",
    "    img = cv2.resize(img_to_array(img), dsize=imsize)\n",
    "    y_train.append(img)\n",
    "y_train = np.array(y_train).astype('uint8')\n",
    "y_train = tf.keras.utils.to_categorical(y_train, dtype='float32')\n",
    "print(y_train.shape)\n",
    "\n",
    "y_test = []\n",
    "for archivo in os.listdir('dataset_sunrgbd/test_label'):\n",
    "    img = np.load(os.path.join('dataset_sunrgbd/test_label',archivo))\n",
    "    img = cv2.resize(img_to_array(img), dsize=imsize)\n",
    "    y_test.append(img)\n",
    "y_test = np.array(y_test).astype('uint8')\n",
    "y_test = tf.keras.utils.to_categorical(y_test, dtype='float32')\n",
    "print(y_test.shape)\n",
    "\n",
    "del img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20717d0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 814. MiB for an array with shape (457, 96, 128, 38) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13008/565149654.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mx1_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mx1_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2197\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2199\u001b[1;33m     return list(chain.from_iterable((_safe_indexing(a, train),\n\u001b[0m\u001b[0;32m   2200\u001b[0m                                      _safe_indexing(a, test)) for a in arrays))\n\u001b[0;32m   2201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2197\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2199\u001b[1;33m     return list(chain.from_iterable((_safe_indexing(a, train),\n\u001b[0m\u001b[0;32m   2200\u001b[0m                                      _safe_indexing(a, test)) for a in arrays))\n\u001b[0;32m   2201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_pandas_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_list_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_array_indexing\u001b[1;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 814. MiB for an array with shape (457, 96, 128, 38) and data type float32"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = np.array(list(zip(x1_test, x2_test)))\n",
    "del x1_test, x2_test\n",
    "\n",
    "x_test, x_val, y_test, y_val = train_test_split(x,y_test,test_size=0.2)\n",
    "\n",
    "x1_test, x2_test = x_test[:, 0], x_test[:, 1]\n",
    "x1_val, x2_val = x_val[:, 0], x_val[:, 1]\n",
    "\n",
    "del x_test, x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4e3e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        shear_range=0.02,\n",
    "        zoom_range=[0.9, 1.25],\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        fill_mode=\"reflect\") #'nearest'\n",
    "val_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09e12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen.fit(x1_train)\n",
    "val_datagen.fit(x1_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070d1331",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_datagen.flow([x1_train, x2_train], y_train, batch_size=16)\n",
    "validation = val_datagen.flow([x1_val, x2_val], y_val, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c3b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_color(ax, color, title=\"Color\"):\n",
    "    \"\"\"Displays a color image from the NYU dataset.\"\"\"\n",
    "\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title)\n",
    "    ax.imshow(color)\n",
    "\n",
    "def plot_depth(ax, depth, title=\"Depth\"):\n",
    "    \"\"\"Displays a depth map from the NYU dataset.\"\"\"\n",
    "\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title)\n",
    "    ax.imshow(depth, cmap='Spectral')\n",
    "\n",
    "def plot_label(ax, labels, title=\"Label\"):\n",
    "    \"\"\"Displays a label map from the NYU dataset.\"\"\"\n",
    "\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title)\n",
    "    ax.imshow(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b0f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(\"Labeled Dataset Sample\", figsize=(24, 10))\n",
    "\n",
    "number = 1\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "plot_color(ax, x1_train[1])\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 2)\n",
    "plot_depth(ax, x2_train[1])\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 3)\n",
    "plot_label(ax, np.argmax(y_train[1],axis=-1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c8d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel():\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Crear el modelos para segmentación\n",
    "        '''\n",
    "        self.input_rgb = tf.keras.layers.Input(shape=(96,128,3),dtype=tf.float32,name='input_1')\n",
    "        self.input_depth = tf.keras.layers.Input(shape=(96,128,1),dtype=tf.float32,name='input_2')\n",
    "        self.fusion_1 = None\n",
    "        self.fusion_2 = None\n",
    "\n",
    "    def rgbd_fusion(self,input_r,input_d,capa):\n",
    "        r = tf.keras.layers.GlobalAveragePooling2D()(input_r)\n",
    "        r = tf.keras.layers.Reshape((1,1,32*capa))(r)\n",
    "        r = tf.keras.layers.Conv2D(2*capa,kernel_size=(1,1),strides=(1,1),activation='relu')(r)\n",
    "        r = tf.keras.layers.Conv2D(32*capa,kernel_size=(1,1),strides=(1,1),activation='sigmoid')(r)\n",
    "        m_1 = tf.keras.layers.Multiply()([input_r,r])\n",
    "        \n",
    "        d = tf.keras.layers.GlobalAveragePooling2D()(input_d)\n",
    "        d = tf.keras.layers.Reshape((1,1,32*capa))(d)\n",
    "        d = tf.keras.layers.Conv2D(2*capa,kernel_size=(1,1),strides=(1,1),activation='relu')(d)\n",
    "        d = tf.keras.layers.Conv2D(32*capa,kernel_size=(1,1),strides=(1,1),activation='sigmoid')(d)\n",
    "        m_2 = tf.keras.layers.Multiply()([input_d,d])\n",
    "        \n",
    "        return tf.keras.layers.Add()([m_1,m_2])\n",
    "\n",
    "    def resnet_layer(self,input_res, input_add, filters):\n",
    "        x = tf.keras.layers.Conv2D(filters, (3, 1), padding='same')(input_res)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(filters, (1, 3), padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(filters, (3, 1), padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(filters, (1, 3), padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "        x = tf.keras.layers.Add()([input_add,x])\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def resnet_layer_div(self,input_res, input_add, filters):\n",
    "        x = tf.keras.layers.Conv2D(filters, (3, 1), padding='same', strides=(2,1))(input_res)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "        \n",
    "        x = tf.keras.layers.Conv2D(filters, (1, 3), padding='same', strides=(1,2))(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(filters, (3, 1), padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(filters, (1, 3), padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "        x = tf.keras.layers.Add()([input_add,x])\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "        return x\n",
    "    \n",
    "    def bottleneck_layer(self,input_bo,filters):\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 1), activation='relu', padding='same')(input_bo)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel_size=(1, 3), activation='relu', padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 1), activation='relu', padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel_size=(1, 3), activation='relu', padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Add()([x, input_bo])\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "    def decode_layer(self,input_de,input_add,filters):\n",
    "        x = tf.keras.layers.Conv2D(filters,kernel_size=(3,3),activation='relu', padding='same')(input_de)\n",
    "        x = self.bottleneck_layer(x, filters)\n",
    "        x = self.bottleneck_layer(x, filters)\n",
    "        x = self.bottleneck_layer(x, filters)\n",
    "        x = tf.keras.layers.UpSampling2D(size=(2,2), interpolation=\"nearest\")(x)\n",
    "        x_2 = tf.keras.layers.Conv2D(filters, kernel_size=(1, 1), activation='relu', padding='same')(input_add)\n",
    "        x = tf.keras.layers.Add()([x, x_2])\n",
    "\n",
    "        return x\n",
    "        \n",
    "    def model(self):\n",
    "\n",
    "        # Preparar la entrada de datos\n",
    "        xr = tf.keras.layers.Conv2D(32,kernel_size=(7,7),strides=(2,2),padding='same',dilation_rate=(1,1),activation='relu')(self.input_rgb)\n",
    "        xd = tf.keras.layers.Conv2D(32,kernel_size=(7,7),strides=(2,2),padding='same',dilation_rate=(1,1),activation='relu')(self.input_depth)\n",
    "\n",
    "        # Fusion\n",
    "        xr = self.rgbd_fusion(xr,xd,1)\n",
    "\n",
    "        # Preparar para encoder\n",
    "        xr = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same')(xr)\n",
    "        xd = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same')(xd)\n",
    "\n",
    "        ''' ENCODER'''\n",
    "        # PRIMERA CAPA\n",
    "        xr = self.resnet_layer(xr,xr,32)\n",
    "        xd = self.resnet_layer(xd,xd,32)\n",
    "        xr = self.resnet_layer(xr,xr,32)\n",
    "        xd = self.resnet_layer(xd,xd,32)\n",
    "        xr = self.resnet_layer(xr,xr,32)\n",
    "        xd = self.resnet_layer(xd,xd,32)\n",
    "        # Fusion despues de Resnet\n",
    "        xr = self.rgbd_fusion(xr,xd,1)\n",
    "        self.fusion_1 = xr  # Aqui guardamos la fusion realizada despues de la capa resnet. Esto lo utilizaremos para decodificar.\n",
    "\n",
    "        # SEGUNDA CAPA\n",
    "        xr = self.resnet_layer_div(xr,tf.keras.layers.Conv2D(64,kernel_size=(1,1),strides=(2,2))(xr),64)\n",
    "        xd = self.resnet_layer_div(xd,tf.keras.layers.Conv2D(64,kernel_size=(1,1),strides=(2,2))(xd),64)\n",
    "        xr = self.resnet_layer(xr,xr,64)\n",
    "        xd = self.resnet_layer(xd,xd,64)\n",
    "        xr = self.resnet_layer(xr,xr,64)\n",
    "        xd = self.resnet_layer(xd,xd,64)\n",
    "        xr = self.resnet_layer(xr,xr,64)\n",
    "        xd = self.resnet_layer(xd,xd,64)\n",
    "        \n",
    "        # Fusion despues de Resnet\n",
    "        xr = self.rgbd_fusion(xr,xd,2)\n",
    "        self.fusion_2 = xr  # Aqui guardamos la fusion realizada despues de la capa resnet. Esto lo utilizaremos para decodificar.\n",
    "\n",
    "        # TERCERA CAPA\n",
    "        xr = self.resnet_layer_div(xr,tf.keras.layers.Conv2D(128,kernel_size=(1,1),strides=(2,2))(xr),128)\n",
    "        xd = self.resnet_layer_div(xd,tf.keras.layers.Conv2D(128,kernel_size=(1,1),strides=(2,2))(xd),128)\n",
    "        xr = self.resnet_layer(xr,xr,128)\n",
    "        xd = self.resnet_layer(xd,xd,128)\n",
    "        xr = self.resnet_layer(xr,xr,128)\n",
    "        xd = self.resnet_layer(xd,xd,128)\n",
    "        xr = self.resnet_layer(xr,xr,128)\n",
    "        xd = self.resnet_layer(xd,xd,128)\n",
    "            \n",
    "        # Fusion despues de Resnet\n",
    "        xr = self.rgbd_fusion(xr,xd,4)\n",
    "\n",
    "        '''DECODER'''\n",
    "        xr = self.decode_layer(xr,self.fusion_2,128)\n",
    "        xr = self.decode_layer(xr,self.fusion_1,64)\n",
    "        \n",
    "        xr = tf.keras.layers.Conv2D(38, kernel_size=(3, 3), activation='relu', padding='same')(xr)\n",
    "        xr = tf.keras.layers.UpSampling2D(size=(2,2), interpolation=\"nearest\")(xr)\n",
    "        xr = tf.keras.layers.Conv2D(38, kernel_size=(1, 1), activation='relu', padding='same')(xr)\n",
    "        xr = tf.keras.layers.UpSampling2D(size=(2,2), interpolation=\"nearest\")(xr)\n",
    "        xr = tf.keras.layers.Conv2D(38, kernel_size=(1, 1), activation='relu', padding='same')(xr)\n",
    "        xr = tf.keras.layers.Conv2D(38, kernel_size=(1, 1), activation='softmax', padding='same')(xr)\n",
    "\n",
    "        return tf.keras.Model(inputs=[self.input_rgb, self.input_depth], outputs=xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c997d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel().model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1fc41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model,to_file=\"model.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc(history, title=\"Model Accuracy\"):\n",
    "    \"\"\"Imprime una gráfica mostrando la accuracy por epoch obtenida en un entrenamiento\"\"\"\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_loss(history, title=\"Model Loss\"):\n",
    "    \"\"\"Imprime una gráfica mostrando la pérdida por epoch obtenida en un entrenamiento\"\"\"\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_compare_losses(history1, history2, name1=\"Red 1\",\n",
    "                        name2=\"Red 2\", title=\"Graph title\"):\n",
    "    \"\"\"Compara losses de dos entrenamientos con nombres name1 y name2\"\"\"\n",
    "    plt.plot(history1.history['loss'], color=\"green\")\n",
    "    plt.plot(history1.history['val_loss'], 'r--', color=\"green\")\n",
    "    plt.plot(history2.history['loss'], color=\"blue\")\n",
    "    plt.plot(history2.history['val_loss'], 'r--', color=\"blue\")\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train ' + name1, 'Val ' + name1, \n",
    "                'Train ' + name2, 'Val ' + name2],\n",
    "               loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_compare_accs(history1, history2, name1=\"Red 1\",\n",
    "                      name2=\"Red 2\", title=\"Graph title\"):\n",
    "    \"\"\"Compara accuracies de dos entrenamientos con nombres name1 y name2\"\"\"\n",
    "    plt.plot(history1.history['accuracy'], color=\"green\")\n",
    "    plt.plot(history1.history['val_accuracy'], 'r--', color=\"green\")\n",
    "    plt.plot(history2.history['accuracy'], color=\"blue\")\n",
    "    plt.plot(history2.history['val_accuracy'], 'r--', color=\"blue\")\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train ' + name1, 'Val ' + name1, \n",
    "                'Train ' + name2, 'Val ' + name2], \n",
    "               loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18403f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = 2. * K.sum(intersection)/ (K.sum(y_true_f) + K.sum(y_pred_f))\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "def bce_logdice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))\n",
    "\n",
    "def weighted_bce_loss(y_true, y_pred, weight):\n",
    "    epsilon = 1e-7\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    logit_y_pred = K.log(y_pred / (1. - y_pred))\n",
    "    loss = weight * (logit_y_pred * (1. - y_true) + \n",
    "                     K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n",
    "    return K.sum(loss) / K.sum(weight)\n",
    "\n",
    "def weighted_dice_loss(y_true, y_pred, weight):\n",
    "    smooth = 1.\n",
    "    w, m1, m2 = weight, y_true, y_pred\n",
    "    intersection = (m1 * m2)\n",
    "    score = (2. * K.sum(w * intersection) + smooth) / (K.sum(w * m1) + K.sum(w * m2) + smooth)\n",
    "    loss = 1. - K.sum(score)\n",
    "    return loss\n",
    "\n",
    "def weighted_bce_dice_loss(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    # if we want to get same size of output, kernel size must be odd\n",
    "    averaged_mask = K.pool2d(\n",
    "            y_true, pool_size=(50, 50), strides=(1, 1), padding='same', pool_mode='avg')\n",
    "    weight = K.ones_like(averaged_mask)\n",
    "    w0 = K.sum(weight)\n",
    "    weight = 5. * K.exp(-5. * K.abs(averaged_mask - 0.5))\n",
    "    w1 = K.sum(weight)\n",
    "    weight *= (w0 / w1)\n",
    "    loss = weighted_bce_loss(y_true, y_pred, weight) + dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def true_positive_rate(y_true, y_pred):\n",
    "    return K.sum(K.flatten(y_true)*K.flatten(K.round(y_pred)))/K.sum(y_true)\n",
    "#seg_model.compile(optimizer=Adam(1e-4, decay=1e-6), loss=dice_p_bce, metrics=[dice_coef, 'binary_accuracy', true_positive_rate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b012e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"{}_best_weights.hdf5\".format('seg_model_2')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, \n",
    "                                   patience=1, \n",
    "                                   verbose=1, min_delta=0.0001, cooldown=2)\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\",\n",
    "                      patience=10) # probably needs to be more patient, but kaggle time is limited\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52946600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models.metrics import iou_score\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=opt, loss=dice_loss, \n",
    "              metrics=[dice_coef, \n",
    "                       'accuracy', \n",
    "                       true_positive_rate, \n",
    "                       iou_score\n",
    "                      ])\n",
    "\n",
    "history = model.fit(train, epochs=1000, verbose=1, validation_data=validation, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9540b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc(history)\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bfc5af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1207818f02c8e106e76b126192fd4af7c5708815ebe301344494fac4b3fd23ae"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
